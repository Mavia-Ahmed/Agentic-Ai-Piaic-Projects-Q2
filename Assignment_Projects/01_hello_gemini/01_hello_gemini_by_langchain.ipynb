{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project 1: LangChain Hello World project**"
      ],
      "metadata": {
        "id": "NA6ECIjLvm88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For creating a simple LangChain Colab Notebook that uses the Google Gemini Flash 1.5 model to answer user questions. This example below is provided to help you get started assumes you have access to the Gemini API and a basic Python environment."
      ],
      "metadata": {
        "id": "lqEIFGhyvudc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Install Required Libraries**"
      ],
      "metadata": {
        "id": "y5_bGvAav0uj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following commands to ensure all required libraries are installed:"
      ],
      "metadata": {
        "id": "kDJA6XxCv7k6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "walAQijZvegw",
        "outputId": "5a2d6d2a-499b-41ae-ce48-1ed038d006c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.14)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.11)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.29)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.5)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.2.10)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.5)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.14)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install google-generativeai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3teBgZV7wW1A",
        "outputId": "92c738f3-aa75-47b3-d25c-12a6eae056e5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.3)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.10)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.155.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.25.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.10.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.10->google-generativeai) (1.25.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.66.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.27.2)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.69.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Import Libraries**"
      ],
      "metadata": {
        "id": "TJtTPwuk1obN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update your imports to include the correct modules for Google Gemini:"
      ],
      "metadata": {
        "id": "dNZX9xqk1ral"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "import google.generativeai as genai\n",
        "from langchain.llms.base import LLM\n",
        "from typing import Optional, List, Mapping, Any"
      ],
      "metadata": {
        "id": "8GuuG-0jwcWc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Set Up the Gemini API**"
      ],
      "metadata": {
        "id": "27AUWiDO1zXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtain your API key from the Google AI Studio or Google Cloud, then configure the Gemini model:"
      ],
      "metadata": {
        "id": "urmDi1u616K1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nEDl23IiwibV",
        "outputId": "3b9d2dce-64c5-4d48-a2e2-5a536730812d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AIzaSyCrhUfm8Ji5sYbgVPxTtr55lf8FeZwpid0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Retrieve the API key securely\n",
        "GEMINI_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Verify if the key is loaded\n",
        "if not GEMINI_API_KEY:\n",
        "    raise ValueError(\"API key not found. Make sure 'GOOGLE_API_KEY' is set in userdata.\")\n",
        "\n",
        "# Initialize the Gemini API\n",
        "genai.configure(api_key=GEMINI_API_KEY)"
      ],
      "metadata": {
        "id": "i4S6tVcixGfV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Create a Custom Wrapper for Google Gemini**"
      ],
      "metadata": {
        "id": "j9cSlGH22BJi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain allows creating a custom LLM class. Here’s the wrapper for Gemini:"
      ],
      "metadata": {
        "id": "bFTeuoSu2GPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GeminiLLM(LLM):\n",
        "    model: str = \"gemini-1.5-flash\"  # Model name\n",
        "    temperature: float = 0.7\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"google_gemini\"\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        model = genai.GenerativeModel(self.model)\n",
        "        response = model.generate_content(prompt, generation_config={\"temperature\": self.temperature})\n",
        "        return response.text\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        return {\"model\": self.model, \"temperature\": self.temperature}"
      ],
      "metadata": {
        "id": "65vGRN7Rx2rx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Integrate with LangChain**"
      ],
      "metadata": {
        "id": "wjxN86q42LRT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can use the GeminiLLM class as a replacement for GoogleGeminiFlash:"
      ],
      "metadata": {
        "id": "k3ZEWFS62Rxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Gemini LLM\n",
        "llm = GeminiLLM(model=\"gemini-1.5-flash\", temperature=0.7)\n",
        "\n",
        "# Create a prompt template\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"You are a helpful assistant. Answer the following question:\\n\\n{question}\"\n",
        ")\n",
        "\n",
        "# Create the LangChain pipeline\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMhEg_B3x7eD",
        "outputId": "5ce3c9e5-1cb7-48a3-8f54-5c38d8f615fd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-8c22c1e77a12>:11: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(llm=llm, prompt=prompt_template)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #Ask a sample question\n",
        "question = \"What is LangChain?\"\n",
        "response = chain.run({\"question\": question})\n",
        "\n",
        "print(\"Answer:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "yY8kG3-0yFJc",
        "outputId": "de067a7f-14df-448e-810a-3b008aa2f4e9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-a3996980d26a>:3: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = chain.run({\"question\": question})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: LangChain is a framework for developing applications powered by large language models (LLMs).  It's designed to make it easier to build LLM-powered applications by providing modular components and tools for:\n",
            "\n",
            "* **Connecting to LLMs:**  LangChain simplifies interacting with various LLMs, abstracting away the specifics of different APIs.  This allows you to easily switch between providers like OpenAI, Hugging Face, etc.\n",
            "\n",
            "* **Managing memory:** LLMs are stateless, meaning they forget previous interactions. LangChain provides mechanisms for adding memory to your applications, allowing the LLM to maintain context across multiple turns in a conversation or a series of tasks.\n",
            "\n",
            "* **Chain building:** This is a core feature.  LangChain lets you chain together multiple LLMs or other components (like prompts, indexes, etc.) to create complex workflows. This enables building sophisticated applications that go beyond simple single-turn interactions.\n",
            "\n",
            "* **Indexing and querying data:**  LangChain allows you to connect LLMs to your own data sources.  This enables the LLM to access and process your private information, making it useful for tasks like question answering over specific documents or personalized chatbot experiences.\n",
            "\n",
            "* **Agent development:**  LangChain provides tools for building agents that can autonomously interact with their environment (e.g., the internet, databases) to complete tasks.  This allows for more advanced applications that can dynamically gather information and make decisions.\n",
            "\n",
            "\n",
            "In essence, LangChain provides a structured way to build more powerful and useful applications leveraging the capabilities of LLMs, moving beyond simple prompt-response interactions towards more complex and context-aware systems.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Next Steps in Colab Project**"
      ],
      "metadata": {
        "id": "pFtGc26vyfpL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Experiment with Prompts:** Add more prompt templates to see how the model responds.\n",
        "*   **Add Memory:** Use LangChain’s memory feature to make the interaction multi-turn.\n",
        "*   **Integrate Tools:** Extend the chain to include tools like database queries or APIs.\n",
        "*   **Explore Gemini Features:** Adjust temperature, max tokens, and other parameters to optimize responses."
      ],
      "metadata": {
        "id": "r_TvYuP2ykUH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Experiment with Prompts**"
      ],
      "metadata": {
        "id": "6hc4STU6zeLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can create multiple prompt templates to see how the model responds to various formats and tasks."
      ],
      "metadata": {
        "id": "N6-6HdqUziev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Template for answering factual questions\n",
        "fact_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"You are a knowledgeable assistant. Answer this factual question:\\n{question}\"\n",
        ")\n",
        "\n",
        "# Template for creative storytelling\n",
        "story_template = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"You are a creative storyteller. Write a short story about the following topic:\\n{topic}\"\n",
        ")\n",
        "\n",
        "# Using a different template\n",
        "question = \"Tell me a short story about a quaid e azam.\"\n",
        "response = LLMChain(llm=llm, prompt=story_template).run({\"topic\": question})\n",
        "print(\"Story Response:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "ybeH-DfhzJGw",
        "outputId": "69d5d309-5cff-49cd-8809-27929914aeb7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Story Response: The year is 1947.  The air in Jinnah’s study hung thick with the scent of jasmine and the weight of a nation’s hope.  He wasn’t the polished, almost mythical figure the world saw; tonight, he was just Muhammad Ali Jinnah, a man weary beyond measure.  The partition riots, a bloody tapestry woven with sorrow and displacement, had left him etched with lines that spoke of sleepless nights and agonizing choices.\n",
            "\n",
            "He sat hunched over a chipped teacup, the lukewarm chai growing cold beside a mountain of official documents.  Outside, the celebratory fireworks crackled – a jarring contrast to the turmoil within.  He’d achieved his life’s ambition, the creation of Pakistan, but the victory felt like ashes in his mouth.\n",
            "\n",
            "A soft cough broke the silence.  His niece, Fatima Jinnah, entered, her face etched with concern.  She offered him a comforting smile, but it didn’t quite reach her eyes.\n",
            "\n",
            "“They’re still fighting, Baba,” she whispered, her voice tight with sorrow.\n",
            "\n",
            "Jinnah nodded, his gaze fixed on a single flickering candle.  He saw not the flame, but the faces of the countless souls caught in the crossfire – Hindus, Muslims, Sikhs, all victims of a hastily drawn boundary.  The dream of a united India, once so vibrant, had shattered into a million fragments.\n",
            "\n",
            "“Fatima,” he said, his voice a low rumble, “they say a leader must be strong, unyielding.  But tonight, I feel…broken.”\n",
            "\n",
            "Fatima gently took his hand, her touch a balm to his weary soul.  “Baba,” she said softly, “you built a nation from the fragments of hope.  The path ahead is difficult, but you gave them a home, a place to call their own.  That is strength enough.”\n",
            "\n",
            "He looked at her, at the unwavering love in her eyes, and a flicker of his old fire rekindled.  He raised his hand, a gesture that had once commanded armies and swayed parliaments, now simply encompassing his niece in a silent promise.\n",
            "\n",
            "He wouldn't let the darkness win.  The task ahead was monumental, the wounds deep, but he would continue to fight for the very essence of Pakistan – the hope of a future free from the bloodshed that had marred its birth.  He would fight for the dream, even if it meant carrying the weight of a broken heart, one weary step at a time.  For he was, after all, Quaid-e-Azam – the Great Leader – and that meant more than just victory; it meant enduring the burden of responsibility, even in the face of unimaginable sorrow.  And he would endure.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Add Memory (Multi-Turn Conversations)**"
      ],
      "metadata": {
        "id": "FRlo8p-nz7bH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain provides memory for maintaining context across multiple interactions."
      ],
      "metadata": {
        "id": "PF2Fxdg40Aqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reuire module installation\n",
        "!pip install langchain-experimental -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8Y0ifpv0FAB",
        "outputId": "11cea00d-5e33-470e-8778-cc2ae13889f2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/209.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m121.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/49.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "# Initialize memory\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Create a conversational chain with memory\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "# Start a conversation\n",
        "response1 = conversation.run(\"What is LangChain?\")\n",
        "print(\"Q1:\", response1)\n",
        "\n",
        "response2 = conversation.run(\"Can you explain it in simple words?\")\n",
        "print(\"Q2:\", response2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "Qzm5FoVc0Yzs",
        "outputId": "2dc5aaea-6563-4a51-c6f2-2ee73507de57"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-0daba4cfee94>:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory()\n",
            "<ipython-input-12-0daba4cfee94>:8: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
            "  conversation = ConversationChain(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q1: LangChain is a framework for developing applications powered by large language models (LLMs).  Think of it as a toolbox filled with different components that you can assemble to create sophisticated LLM-based applications.  Instead of writing everything from scratch, LangChain provides pre-built modules for common tasks, making the development process much faster and easier.\n",
            "\n",
            "Specifically, LangChain offers several key features:\n",
            "\n",
            "* **Modules for interacting with LLMs:**  It handles the communication with various LLMs, abstracting away the specifics of each API.  This means you can easily switch between different providers like OpenAI, Hugging Face, or even your own custom models without modifying much of your code.\n",
            "\n",
            "* **Memory components:**  LLMs are stateless by nature, meaning they forget previous interactions. LangChain provides different memory mechanisms to maintain context across multiple turns in a conversation or a series of tasks. This allows for more coherent and engaging interactions.  Examples of memory types include ConversationBufferMemory (which stores recent conversation history) and ConversationSummaryMemory (which summarizes the conversation).\n",
            "\n",
            "* **Chains:** These are sequences of calls to LLMs or other utilities, allowing you to build complex workflows.  For example, you could create a chain that first extracts information from a document, then uses that information to answer a user's question.  Different chain types include sequential chains, map chains, and more complex, custom-designed chains.\n",
            "\n",
            "* **Agents:**  Agents allow your application to decide which LLMs or tools to use based on the current context.  This is particularly useful for tasks that require interacting with external resources or making decisions based on information retrieval.  An agent might decide to use a search engine, a database, or an LLM depending on the user's request.\n",
            "\n",
            "* **Indexes:** LangChain provides tools to index various data sources, such as documents, PDFs, and websites. This allows you to easily search and query your own data using LLMs.  Different index types are available depending on the nature of your data.\n",
            "\n",
            "\n",
            "In short, LangChain helps you build applications that go beyond simple prompts and responses, enabling more complex and interactive experiences with LLMs.  It's designed to simplify the development process and make building powerful LLM-powered applications more accessible.\n",
            "\n",
            "Q2: Imagine you want to build a robot that can answer questions using a super-smart AI brain (like ChatGPT).  LangChain is a set of LEGO bricks that help you build that robot easily.  Instead of building the whole robot from scratch, you can use LangChain's pre-made parts:\n",
            "\n",
            "* **Brain Connectors:**  These connect to different AI brains (like ChatGPT or others), letting you easily swap them.\n",
            "* **Memory:** This is like the robot's memory; it remembers what you've said before.\n",
            "* **Instructions:**  These are step-by-step guides telling the robot what to do (like \"read this document, then answer this question\").\n",
            "* **Decision Maker:** This helps the robot decide which tool to use (the AI brain, a search engine, etc.) to answer your question best.\n",
            "* **Data Organizers:** These help the robot find information in documents or websites.\n",
            "\n",
            "So LangChain is like a toolbox making it much easier and faster to build complex AI applications.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Explore Gemini Features**"
      ],
      "metadata": {
        "id": "CqNoDfwq0ntA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tune Gemini responses by adjusting parameters like temperature and max_tokens.\n",
        "\n",
        "Temperature: Controls creativity (higher = more creative). Max Tokens: Limits response length."
      ],
      "metadata": {
        "id": "ZBRXgyxJ0qPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust model settings\n",
        "llm = GeminiLLM(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    api_key=userdata.get(\"GOOGLE_API_KEY\"),\n",
        "    temperature=0.9,  # More creative\n",
        "    max_output_tokens=200  # Limit output length\n",
        ")"
      ],
      "metadata": {
        "id": "OWa3p-cc0vOX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Combining It All**"
      ],
      "metadata": {
        "id": "keX-1VwQ061t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine multiple enhancements—like memory, new prompts, and tools—into a full pipeline"
      ],
      "metadata": {
        "id": "7hggNL6u09pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# configure model\n",
        "llm = GeminiLLM(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    api_key=userdata.get(\"GOOGLE_API_KEY\"),\n",
        "    temperature=0.9,  # More creative\n",
        "    max_output_tokens=200  # Limit output length\n",
        ")\n",
        "# Template\n",
        "template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"You are a helpful assistant. Answer this question:\\n{question}\"\n",
        ")\n",
        "\n",
        "# Memory\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Chain with memory\n",
        "conversation = LLMChain(llm=llm, prompt=template, memory=memory)"
      ],
      "metadata": {
        "id": "oJZImyd61Br0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run interactions\n",
        "response1 = conversation.run({\"question\": \"define science?\"})\n",
        "print(response1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "W6S8I8fX1PFQ",
        "outputId": "e22627a7-f68e-4623-9dc5-d7a272b58ef4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Science is a systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.  It's a process of inquiry that involves observation, experimentation, and the formulation of theories to explain natural phenomena.  Key characteristics include:\n",
            "\n",
            "* **Empirical evidence:**  Reliance on observation and experimentation to gather data.\n",
            "* **Testability:**  Theories and hypotheses must be able to be tested and potentially falsified.\n",
            "* **Objectivity:** Striving for unbiased observation and interpretation of results.\n",
            "* **Reproducibility:**  Experiments and findings should be able to be replicated by others.\n",
            "* **Cumulative:**  Scientific knowledge builds upon previous findings and theories.\n",
            "* **Self-correcting:**  Errors and inconsistencies are identified and addressed through further investigation.\n",
            "\n",
            "\n",
            "In short, science is a way of understanding the world through rigorous investigation and evidence-based reasoning.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response2 = conversation.run({\"question\": \"Explain it in a funny way.\"})\n",
        "print(response2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "ZiYQI6ab1WyC",
        "outputId": "027c7c5f-3309-47e6-8130-bf75b1a6d423"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide me with the question you'd like me to answer in a funny way!  I'm ready to unleash my inner comedian (which, let's be honest, is mostly just slightly awkward puns and terrible dad jokes).  Hit me with it!\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project 2: LangChain RAG Project**"
      ],
      "metadata": {
        "id": "aeBISbK9FIYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Install Required Libraries**"
      ],
      "metadata": {
        "id": "dApPWEiNDtZ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   LangChain-Pinecone: For Vector search and document storage.\n",
        "*   LangChain-Google-GenAI: For Embeddings and language model integration.\n",
        "*   Google-Colab: For Drive and Colab-specific functionalities."
      ],
      "metadata": {
        "id": "hCLzry8xD1Yg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2rlJzHLMBQdJ"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install -q langchain-pinecone langchain-google-genai google-colab"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Import Necessary Libraries**"
      ],
      "metadata": {
        "id": "-n3hsVf5rj1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **OS:** For setting environment variables.\n",
        "*   **PineconeVectorStore:** For working with Pinecone's vector database.\n",
        "*   **GoogleGenerativeAIEmbeddings:** For embedding queries.\n",
        "*   **ChatGoogleGenerativeAI:** For getting answers from a generative AI model (e.g., Gemini).\n",
        "*  **UUID:** For generating unique IDs."
      ],
      "metadata": {
        "id": "fSBgs_PusAu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "from google.colab import drive\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain_core.documents import Document\n",
        "from uuid import uuid4"
      ],
      "metadata": {
        "id": "PzKYJBqPx-yW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Load Questions from File**"
      ],
      "metadata": {
        "id": "Da-YQi50s2iR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   This function's purpose is to read any \".txt\" file and convert the questions written in it into a list.\n",
        "*   Each question is processed using strip() to remove trailing whitespace.\n",
        "\n"
      ],
      "metadata": {
        "id": "KYWX56fetsbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File path (Adjust this based on your file location in Google Drive)\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/rag questions.txt'  # Replace with the actual path of your file\n",
        "\n",
        "# Load questions from Notepad file\n",
        "def load_questions(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        questions = file.readlines()\n",
        "    return [q.strip() for q in questions]"
      ],
      "metadata": {
        "id": "3AkEDaPdyRhg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Initialize RAG system components**"
      ],
      "metadata": {
        "id": "9v200zEZuKvk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Set API keys\n",
        "*   Initialize embeddings"
      ],
      "metadata": {
        "id": "5h3wr4NyuQAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize RAG system components\n",
        "def initialize_rag_system():\n",
        "    # Set API keys\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = \"GOOGLE_API_KEY_1\"  # Replace with your Google API Key\n",
        "\n",
        "    # Initialize embeddings\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
      ],
      "metadata": {
        "id": "DFmVz3dPyeEx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Initialize Pinecone and Create Index**"
      ],
      "metadata": {
        "id": "C1rUPAZDugSf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Sign up:** Create an account on Pinecone's website.\n",
        "*   **Get your API key:** Generate an API key from your Pinecone account.\n",
        "*   **Bring it to Colab:** Add that API key to your Google Colab notebook.\n",
        "*   **Run the setup:** Execute the provided code to set up your Pinecone environment."
      ],
      "metadata": {
        "id": "QZfCcCKAu3Oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from google.colab import userdata\n",
        "pinecone_api_key = userdata.get(\"PINECONEKEY1\")\n",
        "\n",
        "pc = Pinecone(api_key=pinecone_api_key)"
      ],
      "metadata": {
        "id": "2RDLRLG8yj7m"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = \"rag-project\"\n",
        "\n",
        "pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=768,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        ")\n",
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "id": "7hS28TJwyrjt"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Embedding Query**"
      ],
      "metadata": {
        "id": "TdeCj4y9vkmv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   The query is converted into embeddings, which are then stored in a vector database.\n",
        "*   This displays the first 5 dimensions of the embeddings in the output."
      ],
      "metadata": {
        "id": "OejtTBx1wNVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY_1')\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "vector = embeddings.embed_query(file_path)\n",
        "vector[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wm8os_n2zKLv",
        "outputId": "b6f39d4a-5ea8-44ff-9a4a-c206e8616e0e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.04573819413781166,\n",
              " -0.026729127392172813,\n",
              " -0.04110725224018097,\n",
              " -0.03643791750073433,\n",
              " 0.008257507346570492]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. Initialize RAG System**"
      ],
      "metadata": {
        "id": "1krT_xZXwf8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Get embeddings ready:** Sets up Google's tools for creating embeddings.\n",
        "*   **Connect to Pinecone:** Links to the Pinecone database where information is stored.\n",
        "*   **Prepare the AI:** Gets the ChatGoogleGenerativeAI model ready to provide answers."
      ],
      "metadata": {
        "id": "A-vaBrq6w1DC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize RAG system components\n",
        "def initialize_rag_system():\n",
        "    # Set API keys using userdata.get\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY_1')  # Retrieve from userdata\n",
        "\n",
        "    # Initialize embeddings\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "    # Initialize Pinecone\n",
        "    from pinecone import Pinecone\n",
        "    # Make sure this is your actual Pinecone API key or retrieve it from userdata\n",
        "    pc = Pinecone(api_key=\"pcsk_2jjqvB_65U7Lh8Q69LVAP9QLrjUMeGqwFEgGhrmdSMoLSdJmwU1f9EogPZ6CthQoyYfZvU\")\n",
        "    index_name = \"rag-project\"\n",
        "    from langchain_pinecone import PineconeVectorStore\n",
        "    vector_store = PineconeVectorStore(index=pc.Index(index_name), embedding=embeddings)\n",
        "\n",
        "    # Initialize LLM (Generative AI model)\n",
        "    llm = ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-1.5-flash\",\n",
        "        temperature=0,\n",
        "        max_tokens=None,\n",
        "        timeout=None,\n",
        "        max_retries=2,\n",
        "    )\n",
        "\n",
        "    return vector_store, llm"
      ],
      "metadata": {
        "id": "ZVrFnDM_3bMV"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8. Vector Search and Answer Generation**"
      ],
      "metadata": {
        "id": "delUj8XwxVS9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Find similar information:** For each question, Pinecone is used to search for the 2 most similar pieces of information in the database.\n",
        "*   **Get the AI's response:** The question and the similar information found are given to the Gemini AI model. Gemini then uses this information to create an answer.\n",
        "*   **Show the results:** The original question and Gemini's answer are displayed in the output.\n",
        "##  **Main** **WorkFlow**\n",
        "\n",
        "\n",
        "*   **Get the questions:** The questions are read in from a \".txt\" file.\n",
        "*   **Set up the system:** The RAG system, which uses AI to answer questions, is prepared.\n",
        "*   **Process and show answers:** The system works through each question, finds answers, and shows them in the Colab notebook's output area."
      ],
      "metadata": {
        "id": "W1mWaZk-x1W6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process each question and print answers\n",
        "def process_questions(questions, vector_store, llm):\n",
        "    for query in questions:\n",
        "        print(f\"\\n**Question**: {query}\")\n",
        "        # Perform vector search\n",
        "        vector_results = vector_store.similarity_search(query, k=2)\n",
        "\n",
        "        # Generate the final answer using LLM\n",
        "        final_answer = llm.invoke(f\"ANSWER THE USER QUERY: {query}, Here are some references: {vector_results}\")\n",
        "        print(f\"**Answer**: {final_answer.content}\")\n",
        "\n",
        "# Main workflow\n",
        "if __name__ == \"__main__\":\n",
        "    # Load questions from the file\n",
        "    questions = load_questions(file_path)\n",
        "    print(\"Questions loaded:\", questions)\n",
        "\n",
        "    # Initialize RAG system\n",
        "    vector_store, llm = initialize_rag_system()\n",
        "\n",
        "    # Process questions and display answers in Colab output\n",
        "    process_questions(questions, vector_store, llm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMQWqYC73qt-",
        "outputId": "34980b79-02e8-4a5c-f38d-b149c6ba20ba"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Questions loaded: ['who dreamed creation of pakistan and when?', 'how mohammad ali jinnah succeed?', 'why mahatma gandhi come in the politics?']\n",
            "\n",
            "**Question**: who dreamed creation of pakistan and when?\n",
            "**Answer**: The creation of Pakistan wasn't the dream of a single person, but rather a culmination of ideas and efforts from many individuals over a considerable period.  However, **Muhammad Ali Jinnah** is widely considered the most prominent figure in the movement for a separate Muslim state.  He articulated the vision and led the Muslim League's efforts to achieve it.\n",
            "\n",
            "While the specific \"dream\" evolved over time, the idea of a separate Muslim homeland gained significant momentum in the early to mid-20th century, culminating in the **Pakistan Resolution (Lahore Resolution) passed in 1940**.  This resolution is generally considered the formal articulation of the demand for a separate Muslim state, marking a key moment in the dream's realization.  Therefore, while Jinnah is the most prominent figure associated with the dream, the dream itself solidified around **1940**.\n",
            "\n",
            "**Question**: how mohammad ali jinnah succeed?\n",
            "**Answer**: Muhammad Ali Jinnah's success in achieving the creation of Pakistan is a complex issue with no single answer.  Several factors contributed:\n",
            "\n",
            "* **Effective Leadership and Political Strategy:** Jinnah was a charismatic and skilled orator, able to unite diverse Muslim groups under the banner of a separate Muslim state. He strategically utilized the existing grievances of Muslims within British India, focusing on the fear of Hindu domination in a unified India.  His political maneuvering and negotiation skills were crucial in navigating the complex political landscape of the time.\n",
            "\n",
            "* **Exploitation of Communal Tensions:** While ethically problematic, Jinnah successfully exploited existing religious and political tensions between Hindus and Muslims.  He capitalized on incidents of communal violence and the perceived inability of the Congress to adequately address Muslim concerns.  This fueled the demand for a separate homeland.\n",
            "\n",
            "* **The Changing Political Landscape:** The weakening of British power in India after World War II created an opportunity for the emergence of independent states.  The British, facing increasing pressure and the costs of maintaining control, were more willing to consider the partition of India as a solution to the escalating conflict.\n",
            "\n",
            "* **Support from the Muslim League:** Jinnah's success was inextricably linked to the strength and organization of the Muslim League, which he effectively led and transformed into a powerful political force.  The League's mobilization of Muslim support was essential in achieving its goals.\n",
            "\n",
            "* **International Context:** The global political climate, including the rise of nationalism and the end of colonialism, provided a favorable environment for the creation of new nation-states.  International pressure also played a role in influencing the British decision to grant independence to India and Pakistan.\n",
            "\n",
            "\n",
            "It's important to note that Jinnah's success came at a significant cost. The partition of India resulted in widespread violence, displacement, and loss of life.  His legacy remains a subject of intense debate and scrutiny, with many questioning the ethical implications of his actions and the long-term consequences of the partition.\n",
            "\n",
            "**Question**: why mahatma gandhi come in the politics?\n",
            "**Answer**: Mahatma Gandhi entered politics as a response to the injustices and discrimination faced by Indians under British colonial rule.  While he initially focused on legal and social reform, the increasingly oppressive policies and the lack of progress through existing channels pushed him towards active political resistance.  He believed in the power of nonviolent resistance (Satyagraha) to achieve self-rule and social justice for all Indians.  His involvement stemmed from a deep commitment to his country's independence and a belief in the moral imperative to fight for equality and freedom.\n"
          ]
        }
      ]
    }
  ]
}